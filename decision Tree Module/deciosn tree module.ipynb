{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decsion Trees are based on three points named Entropy , information gain and giny impurity\n",
    "# Ideal selection of nodes and predicting separate classes requires low entropy , high information gain , and low genie impurity\n",
    "\n",
    "# Entropy is the measure of randomness in the data with the formula H(S) = - sum(p_i * log2(p_i))\n",
    "# information gain computes the diffrence between the entropy after split and and average entropy after split(low entropy high information gain\n",
    "# and high entropy low information gain) IG(S, A) = H(S) - sum(|S_v|/|S| * H(S_v))\n",
    " # where IG(S, A) is the information gain of attribute A on set S, H(S) is the entropy of set S, |S_v| is the number of examples in subset S_v that have value v for attribute A, and |S| is the total number of examples in set S.\n",
    "\n",
    "The summation is taken over all possible values of attribute A. The attribute that maximizes information gain is chosen as the splitting attribute at the current node.\n",
    "\n",
    "To build a decision tree, the data is recursively split based on the attributes that provide the most information gain until a stopping criterion is met, such as all examples in a node belonging to the same class or the tree reaches a certain depth.\n",
    "\n",
    "Information gain is a key concept in decision trees and is used to construct decision trees that are able to make accurate predictions for new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
